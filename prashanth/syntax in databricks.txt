PySpark is a Python API for Apache Spark. Apache Spark is an analytical processing engine for large scale powerful distributed data processing and 
machine learning applications.
PySpark is a Spark library written in Python to run Python application using Apache Spark capabilities, using PySpark we can run applications parallelly 
on the distributed cluster (multiple nodes).

---------------------Features of Pyspark------------------------------------
In-memory computation
Distributed processing using parallelize
Can be used with many cluster managers
Fault-tolerant
Immutable
Lazy evalution
Cache & Persistence
Inbuilt optimization when using Dataframes
Supports ANSI SQL

---------------------Advantages of Pyspark------------------------------------------------------------------------------------------------
Pyspark is a general-purpose, in-memory, distributed processing engine that allows you to process data efficently in a distributed fashion.
100x faster than Hadoop
we can process data from Hadoop HDFS, AWS S3, and many filesystems
processing Realtime data using streaming and kafka
Using PySpark streaming you can also stream files from the file system and also stream from the socket.
It natively has machine learning and graph libraries.


spark.conf.set("spark.sql.files.maxPartitionBytes",2000000)



------------------------------TO CREATE DATAFRAME MANUALLY FROM RDD-------------------------------------------------------------------------
-----> to create dataframe from RDD (In this columns name will be _1 , _2
	*****df1=rdd.toDF()
-----> to create df from RDD and to specify column names
	*****columns = ["language","users_count"]
	     dfFromRDD1 = rdd.toDF(columns)
-----> to create df from RDD using spark session
	*****dfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)



------------------------------TO CREATE DF FROM LIST COLLECTIONS MANUALLY----------------------------------------------
----->to create df from spark session
	*****df=spark.createDataFrame(data).toDF(*columns)
-----> to create df with schema
	***** from pyspark.sql.types import StructType, StructField, StringType, IntergerType
		data2 = [("edla","prashanth",28,"hyderabad",30000),
			  ("rangu","vihaan",15,"jagtial",20000),
			  ("thoomu","pravalika",24,"knr",15000)]
		schema= StructType[(StructField("firstname", StringType(), True),\
					(StructField("lastname", StringType(), True),\
					(StructField("age", IntergerType(), True),\
					(StructField("city", StringType(), True),\
					(StructField("salary", IntergerType(), True)]
		df=spark.createDataFrame(data=data2,schema=schema)





dbutils.help()
dbutils.secrets.help()
dbutils.secrets.listScopes() --- to get list of scope 
dbutils.secret.list(scopename) ---- to get the key values
dbutils.secret.get(scope:'scope_name',key:'key_name') --- to connect the scopename and key

dbutils.widgets.help()
dbutils.widgets.txt('keyword','value') --- to create parameters
dbutils.widgets.get('keyword') ---- to get connect with parameter


dbutils.fs.ls('path') --- to get list of files and directories
------magic command------
      %fs
      ls /path
dbutils.fs.mkdirs("path")  --- to create folders in the given path
dbutils.fs.mount() --- to create mount point
dbutils.fs.unmount() --- to drop mount point
dbutils.fs.mounts() --- to get list of mounts in ADB
dbutils.fs.refreshMounts()--- to refresh mount points if there any connection issue
dbutils.fs.put() --- to write something into file
dbutils.fs.head() --- to read the file
dbutils.fs.cp() --- to copy file or directory over filesystems
dbutils.fs.mv("sourcepath","Targetpath/filename" ) --- to move file or directory over filesystems
dbutils.fs.rm() --- to delete file or directory over file system

-------------------------To CHECK FILES IN CLUSTER LOCAL DIRECTORY------------------------------------------------------
%sh
ls/tmp

-->To read data  
-->To display result
	***df.show(Truncate = False) --- it displays only limited row result & (Truncate = False is optional) to display full result without truncate
	***display(df) --- it displays result in full & also provides visualization options

-->To get schema of df:
	***df.printSchema() --- it fetches schema of df

-->to get total no. of records
	***df.count() ---- it gives total no. of rows

-->to get column names
	***df.columns --- it fetches column names in file

-->to get column names along with datatypes
	***df.dtypes

-->to filter the required data
	***df1=df.filter(condition)

-->to select specific columns
	***df1=df.select(column_name1,column_name2)


----------------------------------------------------------------------------------------------------------------------------------------------------------------------

---------------------------------READING AND WRITING FROM AZURE SQL DB-----------------------------------------------------------------------
-----> URL to create scope: databricksurl#secrets/createScope
-----> get the scope details of username
***** dbutils.secret.get(scope:'scope_name',key:'key_name')
-----> get the scope details of pswd
***** butils.secret.get(scope:'scope_name',key:'key_name')
-----> syntax to connect to sql db using python 
*****  jdbcUrl = "jdbc:sqlserver://{0}:{1};database={2}".format(jdbcHostname, jdbcPort, jdbcDatabase)
connectionProperties = {
  "user" : jdbcUsername,
  "password" : jdbcPassword,
  "driver" : "com.microsoft.sqlserver.jdbc.SQLServerDriver"
}

---get jdbc sql server url from Azure sql db in connection strings option
---jdbcusername should be replaced with dbutils.secret.get(scope:'scope_name',key:'keyvault_username_saved')
--- jdbcPassword should be replaced with dbutils.secret.get(scope:'scope_name',key:'keyvault_pswd_saved')

-----> to read table data from db to dataframe
***** df = spark.read.jdbc(url=jdbcUrl, table="table_name", properties=connectionProperties)
--- in place of table_name we can use query which can be saved in some other variable
e.g.  table_name = "(select * from employees where emp_no < 10008) emp_alias"
	df = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)
	display(df)

-----> to write data into db from df
***** df.write.jdbc(url=jdbcUrl, mode = 'overwrite/append",table="table_name", properties=connectionProperties)





----------------------------------READING AND WRITING DIFFERENT FILE FORMATS----------------------------------------------------------------


--------------------------------------------------Reading & Writing CSV files-------------------------------------------------------------

-->Syntax for reading csv files

	***syntax 1: df = spark.read.csv('path',header = True, inferSchema = True, sep=',', schema = schema 
///either use inferSchema or schema

	***syntax 2: df = spark.read.format("csv").option("header",True).option("inferSchema",True).option("sep",",").schema(schema).load("path")

				###Any one syntax is used to read csv file to DataFrame


-->Syntax for reading tsv files
	***syntax 1: df = spark.read.tsv('path',header = True, inferSchema = True, sep='\t', schema = schema 
///either use inferSchema or schema

	***syntax 2: df = spark.read.format("tsv").option("header",True).option("inferSchema",True).option("sep","\t").schema(schema).load("path")


-->Syntax for reading PIPE csv files
	***syntax 1: df = spark.read.csv('path',header = True, inferSchema = True, sep='|', schema = schema 
///either use inferSchema or schema

	***syntax 2: df = spark.read.format("tsv").option("header",True).option("inferSchema",True).option("sep","|").schema(schema).load("path")


-->User defined schema syntax:

	***from pyspark.sql.types import DataTypes
	schema = StructType([StructField("col_name1", DataType(),True/False), StructField("col_name2", DataType(),True/False), 
	StructField("col_name3", DataType(),True/False)])

///DataTypes are StructType, Struct Field, IntegerType, StringType, TimeStamp etc. (shortcut after typing 2 letters use 'tab' displays datatype suggestions
In above schema if null values allowed then True, if not allowd False but by default it is True(allows null values)



--------------------------------------------------Reading & Writing PARQUET files-------------------------------------------------------------

-->Read parquet file into DataFrame

	***df=spark.read.parquet("path")

	***df1=spark.read.format("parquet").load("path")


-->write DataFrame as parquet file

	***df.write.mode("append"/"overwrite").parquet("output path")
	***df1.write.mode("append"/"overwrite").format("parquet").save("output path")


--------------------------------------------------Reading & Writing JSON files-------------------------------------------------------------

-->Read JSON file into df 

-->single line JSON file
	***df = spark.read.json("path", schema=userdefined schema)
	   df.show()

	***df1 = spark.read.format("json").load("path",schema=userdefined schema)
OR 	   df1 = spark.read.format("json").schema(userdefined_schema).load("path")


-->Write df to JSON 
	***df.write.mode("overwrite"/"append").format("json").save("path")
	***df.write.mode("overwrite"/"append").json("path")


-->Read multiline JSON file to df
	***df = spark.read.json("path", multiLine = True,schema=Userdefined schema) 
	***df1 = spark.read.format("json").option("multiLine",True).schema(userdefined schema).load("path")
in above syntaxes schema= userdefined schema is optional	

-->to change the DataType of column
	***from pyspark.sql.functions import col
	   df2=df1.withColumn("col_name",col["col_name"].cast("modified datatype"))

--------------------------------------------------Reading & Writing AVRO files-------------------------------------------------------------

---> read avro file 
	***df = spark.read.format("avro").load("path")   ///// use this syntax for avro
	***df1=spark.read.avro("filepath") //////// for avro this syntax does not work
--->writing avro file
	***df.write.mode("overwrite"/"append").schema(userdefined schema).format("avro").save("filepath")
		////infered schema by default. schema in syntax is optional, if we need to change or modify default schema, we will use schema(userdefined schema)

---> read orc file
	***df = spark.read.format("orc").load("path")
	***df1=spark.read.orc("filepath")
---> write orc file
	***df.write.mode("overwrite"/"append").schema(userdefined schema).format("avro").save("filepath")
		////infered schema by default. schema in syntax is optional, if we need to change or modify default schema, we will use schema(userdefined schema)


---------------Difference between csv, parquet, avro, orc------------
Properties			csv		Avro		parquet		orc
columnar			No		No		Yes			Yes
Human Readable		Yes		No		No			No
compression data		No		Good		Good			Excellent (if 1Gb data in csv, then same data in parquet,orc,avro take 200 to 250Mb space)
Read or Write		Write		Write		Read			Read    (data can be read quickly & execute query fast in parquet & orc)


-------------How to Read different file formats and write into different file formats---------

//////we can read file in one format and can be written in any format
for e.g file is in csv ---- read file in csv ---- we can write file into csv, parquet, orc, avro

***************df=spark.read.format("csv").option("header",True).load("filepath")
***************df=spark.read.format("orc/avro/parquet").load("filepath")

writing above to different formats
***************df.write.mode("append"/"overwrite").option("header",True).format("csv").save("filepath")
***************df.write.mode("append"/"overwrite").format("parquet/orc/avro").save("filepath")


--------------------------------------------------Reading & Writing EXCEL files-------------------------------------------------------------

----> reading Excel files (to read excel file, firstly we have to install spark - excel library from maven either uploading jar file or directly in cluster)
	---------------by default it will read the first sheet in documents
	****df=spark.read.format("com.crealytics.spark.excel").option("header",True).option("inferSchema",True).load("path")
	---------------to read specific sheet in file	                                   	
	****df=spark.read.format("com.crealytics.spark.excel").option("header",True).option("inferSchema",True).option("dataAddress","'sheetname'!").load("path")
	---------to read specific cells in excel...mention cell no digonally----
	****df=spark.read.format("com.crealytics.spark.excel").option("header",True).option("inferSchema",True).option("dataAddress","'sheetname'!A5:B10").load("path")
-----> Writing excel files
	*****df.write.mode("overwrite/append").option("header",True).format("com.crealytics.spark.excel").option("dataAddress","'sheetname'!A5:B10").save("path")


--------------------------------------------------Reading & Writing XML files-------------------------------------------------------------

----> reading XML files (to read XML file, firstly we have to install spark - XML library from maven either uploading jar file or directly in cluster)

	*****df=spark.read.format("com.databricks.spark.xml").option("rootTag","rootfilename").option("rowTag","rowname").schema(s).load("path")
	*****df=spark.read.format("xml").option("rootTag","rootfilename").option("rowTag","rowname").schema(s).load("path")

----->to write XML files to dataframes
	*****df.write.mode("append"/"overwrite").format("com.databricks.spark.xml").option("rootTag","rootfilename").option("rowTag","rowname").save("path")
	*****df.write.mode("append"/"overwrite").format("xml").option("rootTag","rootfilename").option("rowTag","rowname").save("path")


--------------------------------------------------Reading & Writing ZIP, URL, GZ files-------------------------------------------------------------

----> To read data from ZIP files, first we have import zip file from storage to local file system
	***** dbutils.fs.mv("sourcepath","flie:/tmp/zipfilename.zip")   ----file will be moved to local file system
	
	***** %sh                    ---- sh means shell script
		ls/tmp ---- to get the list of files in local file system of cluster

	***** import zipfile 
		with zipfile.Zipfile("/tmp/zipfilename.zip","r") as zip_ref:
			zip_ref.extractall("/tmp/zipdataoutput")

-----> After extracting data from zipfile to local file system, then again we have to move extracted data to storage account
	***** dbutils.fs.mv("file:/tmp/zipdataoutput","dbfs:/mnt/containername/foldername",True)  ----here True is used for multiple files in zip folder to copy one after another

-----> if we dont know the format of data given in zipfile, first we have to read the sample data as per requirement
	***** cs.textFile("dbfs:/mnt/containername/foldername).collect()   ---here collect is used to view total data in files
	***** cs.textFile("dbfs:/mnt/containername/foldername).take(5)	----here take is used to view few data as sample to know the format of data




----> To read data from GunZip files, first we have to import gz file from storage to local file system
	***** dbutils.fs.mv("dbfs:/mnt/containername/sourcefolder/filename.gz","file:/tmp/gzdata.gz")
----> extract data from gunzip file to local file system
	***** %sh
		gunzip -r /tmp/gzdata.gz  
----> after extract data, again move data to storage account
	***** dbutils.fs.mv("file:/tmp/gzdata","dbfs:/mnt/containername/targetfoldername/filename")
----> we dont know data format, so we have to read some data to know that
	***** cs.textFile("dbfs:/mnt/containername/targetfoldername/filename").take(3)




-----> to read data from URL
	***** import urllib.receive as urlreceive
		urlreceive.urlretrieve("URL link.gz/zip/csv/parquet","file:/tmp/filename.gz/zip/csv/parquet")
-----> after importing data from URL, use the same method as zip or gunzip file
		








------------------------------------------------ TRANSFORMATION syntaxes in Databricks---------------------------------------------------------

-->TO CHANGE THE DATATYPE OF COLUMNS:

-->to change the DataType of single column

	***from pyspark.sql.functions import col
	   from pyspark.sql.types import * or (most preferable in real time is enter each datatypes like StructType, StructField, StringType, IntergerType,LongType etc)
	   df2=df1.withColumn("col_name",col/df1["col_name"].cast("modified datatype"))
#### try col once and once try with Df_name df1...verify which syntax is correct

-->Change datatype of multiple columns:
	
	***df2=df1\
		.withColumn("col_name1",col/df1["col_name1"].cast("modified datatype")) \
		.withColumn("col_name2",col/df1["col_name2"].cast("modified datatype")) \
		.withColumn("col_name3",col/df1["col_name3"].cast("modified datatype")) 	so on
	###to check use df2.printSchema()
		
--> Using DataFrame.select() changing required columns
	***df2 = df1.select( df1.col_name1, df1.col_name2, (df1.col_name3.cast(datatype)).alias ("col_name3"), (col_name4.cast(datatype)).alias('column name'))
		###in above we use alias, when we use alias we can also change column name in the df2 as per our choice

--> Changing multiple columns to the same datatype.
	
	***df2=df1.select([df1.cast(datatype)).alias('c') for c in df1.columns])
		###in above we assign all columns same datatype. At first we pass datatype then using "for loop" and in condition,
 		it creates list of columns in df1.These cloumns will be reflected in df2 with same datatype schema.

-->User defined schema syntax:

	***from pyspark.sql.types import DataTypes
	   schema = StructType([StructField("col_name1", DataType(),True/False), StructField("col_name2", DataType(),True/False), 
	   StructField("col_name3", DataType(),True/False)])

DataTypes are StructType, Struct Field, IntegerType, StringType, TimeStamp etc. (shortcut after typing 2 letters use 'tab' displays datatype suggestions
In above schema if null values allowed then True, if not allowd False but by default it is True(allows null values)

-->Changing multiple columns to the different datatypes.
	***from pyspark.sql.types import DataTypes
	   schema = StructType([StructField("col_name1", DataType(),True/False), StructField("col_name2", DataType(),True/False), 
	   StructField("col_name3", DataType(),True/False)])
	***df2=df1.select([df1.cast(schema).alias('c_name) for c_name in df1.columns])
		### schema in above syntax is schema from userdefined schema syntax in previous command...we have to declare schema before this command
-->Using spark.sql()
	***df1.createOrReplaceTempView("df1_temp")
	   df2=spark.sql(''' 
	   select col_name1,modifieddatatype(col_name2),modifieddatatype(col_name3) ... from df1_temp''')









